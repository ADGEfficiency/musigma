## Meetup 13.11.2019

Beginn at 19:00

### Organizational stuff
* Voting mechanism works good and we will stick to it
* Papers that were not chosen for three times should be considered to be archived and/or removed from the vote

### Paper discussion
Paper: [A Short Introduction to Model Selection, Kolmogorov Complexity and Minimum Description Length (MDL) (Nannen, 2003)](http://volker.nannen.com/pdf/short_introduction_to_model_selection.pdf)

#### First thoughts on the paper
* Argument: "One of the best definition of a good model that we have seen in the literature (albeit not flawless)"
* Some explanations are a bit short and/or unnecessary difficult
* The paper was a "mini-version of a PhD-thesis", so it is very dense
* Possible mistakes and contradictions? See the discussion later

#### What is a good model (p. 6-7)?
* (see the page 7 for the author's criteria... should we add some? Are they legit?)
* Add "Allowing to make a causal inference"?
* (Not stated explicitly in the paper, but from the software engineer perspective: Readable/understandable code, low computing time, etc.)
* How to decide to evaluate if the model is good (in the real work setting)?
  * "Cross validation, cross validation, cross validation"
  * Two levels of CV, although very (computationally) expensive

* Why a model that would perform better than a TM should be more complex? Should not it be the reverse?

* Examples of the models
  * The sum of the sin(x) models (e.g., neurons -> EEG) are better predicted even by a linear approximation?
  * In the paper, the author gives the following book for such examples: David Ruelle (1989). Chaotic Evolution and Strange Attractos.

* Did author really assume (only) i.i.d? Namely, would it suffice?
  * Would not be enough (esp. when he introduces AIC)?

#### What is (model) complexity? Is author's definition of complexity 
* Classical definition: The number of free parameters/features
* Assume two models fitted on the same data. One has more parameters -> it is more complex (again, in the traditional definition of complexity)
    * (an illustration on a blackboard that a less complex model can fit arbitrary data WORSE)
    * BUT: Maybe we should use the ability of fit an arbitrary data (measured by ?

* After you fitted your data, ie, fixed the parameters on the given data, the idea of complexity ...?
  * Covariance of the data point with its own prediction becomes 0, since ...

[comment]: <> (Max, please finish this section ðŸ™‚)
* Author would "penalize" the probability of the model in the Bayes' formula
* BUT we should punish the ....?
* OR does not tell about the probability of the 
#### A flaw in the paper?
* "The big and obvious problem with Bayes' rule..." (page 10)
* Occam's razor does NOT say that simple models are more likely to be true
  * Rather: If under given data, two models that predict data similarly good, we should go 
* SO: It is not the right point in the Bayes' theorem where complexity kicks in, so Occam's razor should not be applied there  

#### Conclusion of the paper

* A good remark on the point of real overfitting
  * A case study from one of our participants: Colleagues did a model on a data set with, say, approx 50k variables but only with ~50 data points

#### Notes
